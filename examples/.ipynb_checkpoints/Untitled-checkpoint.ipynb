{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0458bc8e-5074-481f-a183-8a1eb0945adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI(api_key = 'sk-proj-P7nUJmwtobUijXk0oUyuT3BlbkFJZt6h2RJxBYrvB8ltdzsF')\n",
    "\n",
    "assistant = client.beta.assistants.create(\n",
    "  name=\"question_generator\",\n",
    "  instructions=\"You are a helpful assistant designed to output JSON.\",\n",
    "  model=\"gpt-4o\",\n",
    "  tools=[{\"type\": \"file_search\"}],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2982a76f-9200-40a9-9a7e-48d9b913bb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "FileCounts(cancelled=0, completed=1, failed=0, in_progress=0, total=1)\n"
     ]
    }
   ],
   "source": [
    "# Create a vector store caled \"Lecture Slides\"\n",
    "vector_store = client.beta.vector_stores.create(name=\"Lecture Slides\")\n",
    " \n",
    "# Ready the files for upload to OpenAI\n",
    "file_paths = [\"/Users/p70089410/surfdrive/teaching/GLO2221/2324/Gavin/Lecture 7/GLO2221 - Methods - Lecture 7.pdf\"]\n",
    "file_streams = [open(path, \"rb\") for path in file_paths]\n",
    " \n",
    "# Use the upload and poll SDK helper to upload the files, add them to the vector store,\n",
    "# and poll the status of the file batch for completion.\n",
    "file_batch = client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "  vector_store_id=vector_store.id, files=file_streams\n",
    ")\n",
    "\n",
    "# You can print the status and the file counts of the batch to see the result of this operation.\n",
    "print(file_batch.status)\n",
    "print(file_batch.file_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ace16e9-a98b-4531-ba53-b344e26db27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.update(\n",
    "  assistant_id=assistant.id,\n",
    "  #tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}},\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": []}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9a942d99-5d14-4be1-a429-c3ce0184b106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ToolResourcesFileSearch(vector_store_ids=['vs_T4wQ2Vrq6xolQIPIAbvctU3q'])\n"
     ]
    }
   ],
   "source": [
    "# Upload the user provided file to OpenAI\n",
    "message_file = client.files.create(\n",
    "  file=open(\"/Users/p70089410/surfdrive/teaching/GLO2221/2324/Gavin/Lecture 7/GLO2221 - Methods - Lecture 7.pdf\", \"rb\"), purpose=\"assistants\"\n",
    ")\n",
    " \n",
    "# Create a thread and attach the file to the message\n",
    "thread = client.beta.threads.create(\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Generaten two questions for me that I can ask in a statistics exam about linear regression based on the pdf files that I provided. It should be a single choice format. For each question give me three possible answers. Only one must be correct. Indicate the correct answer.\",\n",
    "      # Attach the new file to the message.\n",
    "      \"attachments\": [\n",
    "       #{ \"file_id\": message_file.id, \"tools\": [{\"type\": \"file_search\"}] }\n",
    "      ],\n",
    "      \n",
    "    }\n",
    "  ],\n",
    "  tool_resources={\"file_search\": {\"vector_store_ids\": [vector_store.id]}}\n",
    ")\n",
    " \n",
    "# The thread now has a vector store with that file in its tool resources.\n",
    "print(thread.tool_resources.file_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c739e303-0041-4b4d-baf4-918b00f40301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here are two single-choice questions based on linear regression from the provided documents:\n",
      "\n",
      "### Question 1:\n",
      "In the context of simple linear regression, what does the R-squared value indicate?\n",
      "\n",
      "1. **The average value of the dependent variable when all independent variables are zero.**\n",
      "2. **The proportion of the variation in the dependent variable that is explained by the independent variable(s) in the model.**\n",
      "3. **The probability that the slope of the regression line is zero.**\n",
      "\n",
      "**Correct Answer: 2**[0]\n",
      "\n",
      "### Question 2:\n",
      "Which of the following is a correct interpretation of the slope coefficient in a simple linear regression model?\n",
      "\n",
      "1. **The change in the dependent variable for a one-unit change in the independent variable.**\n",
      "2. **The value of the dependent variable when the independent variable is zero.**\n",
      "3. **The proportion of total variation in the dependent variable that is explained by the regression model.**\n",
      "\n",
      "**Correct Answer: 1** \n",
      "[0] GLO2221 - Methods - Lecture 7.pdf\n"
     ]
    }
   ],
   "source": [
    "# Use the create and poll SDK helper to create a run and poll the status of\n",
    "# the run until it's in a terminal state.\n",
    "\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id, assistant_id=assistant.id\n",
    ")\n",
    "\n",
    "messages = list(client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "\n",
    "message_content = messages[0].content[0].text\n",
    "annotations = message_content.annotations\n",
    "citations = []\n",
    "for index, annotation in enumerate(annotations):\n",
    "    message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "    if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "        cited_file = client.files.retrieve(file_citation.file_id)\n",
    "        citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "\n",
    "print(message_content.value)\n",
    "print(\"\\n\".join(citations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19373e36-d6c1-4e02-9c28-a98583216525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<openai.resources.beta.threads.threads.Threads at 0x103b28f50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tmp = client.beta.vector_stores.list()\n",
    "#client.beta.vector_stores.delete(tmp.data[0].id)\n",
    "#client.beta.vector_stores.list()\n",
    "#vs_N5v29kNE1ZX7XsAZOx31UYLd\n",
    "#vs_nVDPxSERyZ70JRqFtMaFQwBd\n",
    "\n",
    "client.beta.threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4b99d08a-1e02-4c79-947d-4613a2944ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "class Gpt:\n",
    "    def __init__(self):\n",
    "        self.client = None\n",
    "        self.assistant_id = None\n",
    "\n",
    "    def check_assistant_ready(self):\n",
    "        if (self.client is None) or (self.assistant_id is None):\n",
    "            Exception(\"Gpt not ready. Re-run power_up(api_key) to fix the issue.\")\n",
    "\n",
    "    def check_client_ready(self):\n",
    "        if self.client is None:\n",
    "            Exception(\"Client not ready. Re-run power_up(api_key) to fix the issue.\")\n",
    "        \n",
    "    def power_up(self, api_key, assistant_id = None):\n",
    "        self.client = OpenAI(api_key = api_key)\n",
    "        if assistant_id is None:\n",
    "            self.create_assistant()\n",
    "        else:\n",
    "            self.assistant_id = assistant_id        \n",
    "        \n",
    "    def shut_down(self):\n",
    "        pass\n",
    "        \n",
    "    def create_assistant(self, reuse = True):\n",
    "        assistants = self.client.beta.assistants.list().data\n",
    "        results = [assistant for assistant in assistants if assistant.name == \"question_generator\"]\n",
    "        if len(results) > 0 and reuse:\n",
    "            assistant = results[0]\n",
    "        else:\n",
    "            assistant = self.client.beta.assistants.create(\n",
    "                name=\"question_generator\",\n",
    "                instructions=\"You are a helpful assistant designed to output JSON.\",\n",
    "                model=\"gpt-4o\",\n",
    "                tools=[{\"type\": \"file_search\"}],\n",
    "            )\n",
    "        self.assistant_id = assistant.id\n",
    "        return self.assistant_id\n",
    "\n",
    "    def delete_assistant(self, assistant_id):\n",
    "        assistants = self.client.beta.assistants.list().data\n",
    "        for assistant in assistants:\n",
    "            if assistant.id == assistant_id:\n",
    "                self.client.beta.assistants.delete(assistant.id)\n",
    "        if assistant_id == self.assistant_id:\n",
    "            self.assistant_id = None\n",
    "    \n",
    "    def flush_assistants(self):\n",
    "        self.check_client_ready()\n",
    "        assistants = self.client.beta.assistants.list().data\n",
    "        for assistant in assistants:\n",
    "            if assistant.name == \"question_generator\":\n",
    "                self.client.beta.assistants.delete(assistant.id)\n",
    "        self.assistant_id = None\n",
    "        \n",
    "    def create_vector_store(self):\n",
    "        self.check_client_ready()\n",
    "        vector_store = self.client.beta.vector_stores.create(name=\"slides\")\n",
    "        return self.vector_store_id\n",
    "        \n",
    "    def delete_vector_store(self, vector_store_id):\n",
    "        self.check_client_ready()\n",
    "        for vector_store in self.client.beta.vector_stores.list().data:\n",
    "            if (vectore_store.id == vector_store_id):\n",
    "                self.client.beta.vector_stores.delete(vector_store.id)\n",
    "\n",
    "    def list_vector_stores(self, names_only = False):\n",
    "        self.check_client_ready()\n",
    "        vector_stores = self.client.beta.vector_stores.list().data\n",
    "        vector_store_names = [vector_store.name for vector_store in vector_stores]\n",
    "        if names_only:\n",
    "            return vector_store_names\n",
    "        else:\n",
    "            return vector_stores\n",
    "\n",
    "    def flush_vector_stores(self):\n",
    "        self.check_client_ready()\n",
    "        for vector_store in self.client.beta.vector_stores.list().data:\n",
    "            self.client.beta.vector_stores.delete(vector_store.id)\n",
    "        \n",
    "    def list_files(self, names_only = False):\n",
    "        self.check_client_ready()\n",
    "        files = self.client.files.list().data\n",
    "        filenames = [file.filename for file in files]\n",
    "        if names_only:\n",
    "            return filenames\n",
    "        else:\n",
    "            return files\n",
    "            \n",
    "    def flush_files(self):\n",
    "        self.check_client_ready()\n",
    "        files = self.client.files.list().data\n",
    "        for file in files:\n",
    "            self.client.files.delete(file.id)\n",
    "        \n",
    "    def upload_files(self, file_paths):\n",
    "        self.check_client_ready()\n",
    "        \n",
    "        message_files = [self.client.files.create(file = open(path, \"rb\"), purpose = \"assistant\") for path in file_paths]\n",
    "\n",
    "    def upload_file_to_vectore_store(self, file_paths, vector_store_id = None):\n",
    "        file_streams = [open(path, \"rb\") for path in file_paths]\n",
    "        \n",
    "        file_batch = self.client.beta.vector_stores.file_batches.upload_and_poll(\n",
    "            vector_store_id=vector_store_id, files=file_streams\n",
    "        )\n",
    "    \n",
    "    def completion_request(self, messages):\n",
    "        self.check_assistant_ready()\n",
    "        \n",
    "        completion = self.client.chat.completions.create(\n",
    "            model = \"gpt-4o\",\n",
    "            response_format = {\"type\": \"json_object\"},\n",
    "            messages = messages\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "        \n",
    "    def assistant_request(message_content, file_ids = [], vector_store_ids):\n",
    "        self.check_assistant_ready()\n",
    "        \n",
    "        attachements = [{ \"file_id\": file_id, \"tools\": [{\"type\": \"file_search\"}]} for file_id in file_ids]\n",
    "        thread = self.client.beta.threads.create(\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": message_content,\n",
    "                    \"attachments\": attachements,\n",
    "                }\n",
    "            ],\n",
    "            tool_resources={\"file_search\": {\"vector_store_ids\": vector_store_ids}},\n",
    "        )\n",
    "        run = self.client.beta.threads.runs.create_and_poll(\n",
    "            thread_id = thread.id, assistant_id = self.assistant_id\n",
    "        )\n",
    "        \n",
    "        messages = list(self.client.beta.threads.messages.list(thread_id=thread.id, run_id=run.id))\n",
    "        \n",
    "        message_content = messages[0].content[0].text\n",
    "        annotations = message_content.annotations\n",
    "        citations = []\n",
    "        for index, annotation in enumerate(annotations):\n",
    "            message_content.value = message_content.value.replace(annotation.text, f\"[{index}]\")\n",
    "            if file_citation := getattr(annotation, \"file_citation\", None):\n",
    "                cited_file = self.client.files.retrieve(file_citation.file_id)\n",
    "                citations.append(f\"[{index}] {cited_file.filename}\")\n",
    "        \n",
    "        return message_content.value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c90ed7d0-f216-4041-874e-e314a87f4bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = Gpt()\n",
    "tmp.power_up(api_key = 'sk-proj-P7nUJmwtobUijXk0oUyuT3BlbkFJZt6h2RJxBYrvB8ltdzsF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d7542aeb-0933-413c-afa2-38e7338ace33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VectorStore(id='vs_kXWFft6srtnwKXZ8Lv5PzsHl', created_at=1727099563, file_counts=FileCounts(cancelled=0, completed=0, failed=0, in_progress=0, total=0), last_active_at=1727099563, metadata={}, name='slides', object='vector_store', status='completed', usage_bytes=0, expires_after=None, expires_at=None)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.list_vector_stores()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
